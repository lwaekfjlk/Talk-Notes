# A Decade of Machine Learning Accelerator: Lessons Learned and Carbon Footprint

David Patterson from Berkeley

### Introduction to the TPU Family

TPUv1 ~80X of Haswell CPU

â€‹			  ~30X of K80 GPU

TPUv1 paper ~4000 citations

5 Generation of TPU -- 2 generations for inference-only and 3 generations for Training and Inference

### 10 Lessons for ML Accelerator

* Lesson 1: DNN Model Growth

* Lesson2: DNN Workloads Evolve with DNN Breakthroughs
  * DSA needs to be flexible enough to handle different models

* Lesson3: Can optimize DNN as well as compiler and hardware

* Lession4: Inference SLO Limit is Latency, Not Batch size
  * some accelerator claim batch size must be 1 to keep latency low
  * Service-Level Objectives (SLO)

* Lesson5: Production Inference Needs Multi-tenancy
  * Many inferencing applications need to support different inference models

* Lesson6: It is the memory, stupid! (not the FLOP)
  * Easy to scale up FLOP by adding many ALUs to balance the energy of memory accesses
  * Going to DRAM instead of on-clip will cost x100 energy
  * Easier to Scale FLOPs/sec as Logic improves quickest
* Lesson7: DSA Challenge: Optimize for the domain while flexible
* Lesson8: Logic, Wires, SRAM, and DRAMS improve inequality

* Lesson9: Maintain compiler optimizations and ML compatibility
  * XLA(accelerated linear algebra) compiler does whole-program analysis and optimization
  * XLA exploits huge parallelism represented in a Tensorflow input dataflow graph
* Lesson10: Design for performance/TCO vs Perf/CapEx

### 4M for Efficient ML

* **M**odel: Transformer to Evolved Transformer can be 1.3x less energy
* **M**achine: P100 to TPUv2 can be 5.7x less energy
* **M**echanization: PUE from global average to Google average is 1.4x less energy
* **M**aps: Avg %Caron Free Energy to Google Iowa is 6.7x less energy







